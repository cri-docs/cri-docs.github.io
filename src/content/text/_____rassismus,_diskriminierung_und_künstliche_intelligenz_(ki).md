---
title: "Rassismus, Diskriminierung und Künstliche Intelligenz (KI)"
short_title: "Rassismus, Diskriminierung und Künstliche Intelligenz (KI)"
slug: "rassismus,_diskriminierung_und_künstliche_intelligenz_(ki)"
order: 170
index: 11
color: "default"
toc:
---

Text von Pierre-Louis Blanchard (Wissenschaftlicher Mitarbeiter in der «Datenbereinigung» im SNM)  

Wer heute von Künstlicher Intelligenz (KI)[[205]](#footnote-206) spricht, meint typischerweise Anwendungen, die auf maschinellem Lernen basieren. Dabei kommen künstliche neuronale Netze (Deep Learning) zum Einsatz. Das sind rechnergestützte mathematische Modelle, die mit grossen Mengen an Trainingsdaten «gefüttert» werden. Diese Trainingsdaten können Texte, Bilder oder andere Informationen sein, je nach Anwendungsbereich der KI. Eine KI wird «trainiert», indem sie versucht, Probleme mit bekannten Lösungen mit Hilfe statistischer Muster und anhand von (Trainings)Daten zu bearbeiten. Sie unternimmt eine extrem grosse Anzahl an Versuchen, bis sie eine passende Lösung gefunden hat. Die KI «lernt» auf diese Weise selbst, das Problem zu bewältigen – im Gegensatz zum herkömmlichen Programmieren, wo Lösungswege direkt von Menschen einprogrammiert werden. Deshalb spricht man von maschinellem Lernen. Ist eine KI erst einmal trainiert, können neue Probleme anhand der gelernten Muster gelöst werden. Diese Technik eröffnet vielfältige Anwendungsmöglichkeiten, zum Beispiel das Erkennen und Erzeugen von Texten, Bildern oder Sprache und Gesprochenem, autonomes Fahren oder Spamfilter, um nur einige zu nennen. Eine KI denkt nicht und hat kein Verständnis für Ethik oder Moral. Sie spiegelt nur die Muster wider, die in den Trainingsdaten enthalten sind. Grosse KI-Modelle von Unternehmen wie OpenAI, Meta oder Google greifen auf riesige Mengen an Daten aus dem gesamten Internet zurück, einschliesslich grosser digitaler Archive wie dem Wikiverse, dem Internet Archive oder Google Books– und damit auch auf deren problematischen Seiten. Da Rassismus und Diskriminierung allgegenwärtige gesellschaftliche Realitäten sind, spiegeln sich diese Muster – offen oder latent – auch in diesen Quellen wider. Eine unkritische Nutzung von KI kann daher dazu führen, dass rassistische und diskriminierende Strukturen und Ideologien fortgeschrieben werden. Diese können auch in strukturellen Mustern in Erscheinung treten, die schwer zu erkennen sind. Ein Beispiel für latente, diskriminierende Inhalte sind KI-erstellte Bilder, in denen bestimmte Berufe oder Tätigkeiten stereotypisch Personen eines bestimmten Geschlechts oder einer bestimmten Hautfarbe zugeordnet werden.[[206]](#footnote-207) Um dem entgegenzuwirken, werden bereits heute Regulationsmassnahmen eingesetzt.[[207]](#footnote-208) Automatische Programme (Bots) rufen beim sogenannten «scraping» GLAM-Daten z.B. über Online-Sammlungen und Archive oder Drittanbieter wie Wikimedia ab. So fliessen auch diese Daten in die KI-Modelle ein. Damit haben GLAM-Institutionen bei der Zurverfügungstellung ihrer Daten eine besondere Verantwortung, da auch ihre Bestände potenziell rassistische oder anderweitig diskriminierende Inhalte aufweisen können. Zum Beispiel können historische Fotografien mit Bildunterschriften und -beschreibungen auf vielfältige Weise rassistisch oder diskriminierend sein, sei es latent durch dargestellte Machtstrukturen zwischen dargestellten Personen, oder offen durch rassistische und andere diskriminierende Begriffe und Ausdrücke in Bildbeschreibungen. Durch das Eingreifen beim Training von KI-Modellen oder durch eine rassismuskritische Überarbeitung der Datensätze, kann der Fortschreibung von Rassismus und Diskriminierung entgegengewirkt werden. Die KI kann aber auch als Werkzeug genutzt werden, um rassistische und diskriminierende Inhalte in Daten aufzuspüren. Diese Nutzung von KI findet in vereinzelten Fällen schon Anwendung, so zum Beispiel mit dem *DE-BIAS*-Tool der Europeana, mit dem dank KI-Unterstützung nach sensiblen Begriffen in Texten gesucht werden kann.[[208]](#footnote-209)